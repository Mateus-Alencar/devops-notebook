# Conceitos de Orquestra√ß√£o üé≠üì¶

Orquestradores de Containers s√£o respons√°veis pela implementa√ß√£o, monitoramento e controle de containers em ambientes de m√∫ltiplos servidores. Eles s√£o essenciais para ambientes empresariais de hospedagem de aplica√ß√µes.

Hoje existem diversos Orquestradores de Containers dispon√≠veis tanto na nuvem quanto para instala√ß√µes on-premises, ou locais. Entre os principais est√£o: Kubernetes, Docker Swarm, Openshift e Mesos.

No caso dos Orquestradores de Containers, um projeto √© em geral definido de maneira declarativa e √© chamado de Estado do Cluster. Basicamente n√≥s decidimos qual √© o estado desejado do ambiente e o Orquestrador ir√° monitorar todo o ambiente fazendo modifica√ß√µes para alcan√ßar o estado desejado.

## Kubernetes üåê‚Äã

![alt text](image-3.png)

O Orquestrador de Containers mais utilizado no mercado atualmente √© o Kubernetes. Kubernetes (K8s) √© um produto Open Source utilizado para automatizar a implanta√ß√£o, o dimensionamento e o gerenciamento de aplicativos em cont√™iner.

[Documenta√ß√£o Oficial do Kubernetes](https://kubernetes.io/pt-br/docs/home/)

## O Container Engine

  Antes de come√ßar a falar um pouco mais sobre o Kubernetes, n√≥s primeiro precisamos entender
alguns componentes que s√£o importantes no ecossistema do Kubernetes, um desses componentes
√© o Container Engine.
  O Container Engine √© o respons√°vel por gerenciar as imagens e volumes, ele √© o respons√°vel
por garantir que os os recursos utilizados pelos containers est√£o devidamente isolados, a
vida do container, storage, rede, etc.
  At√© pouco tempo atr√°s tinhamos somente o Docker para esse papel. Mas hoje j√° temos diversas
op√ß√µes para se utilizar como Container Engine.
  Op√ß√µes como o Docker, o CRI-O e o Podman s√£o bem conhecidas e preparadas para o ambiente 
produtivo. O Docker, √© o Container Engine mais popular e ele utiliza como Container Runtime o containerd.

#### OCI - Open Container Initiative

A OCI √© uma organiza√ß√£o sem fins lucrativos que tem como objetivo padronizar a cria√ß√£o de containers, para que possam ser executados em qualquer ambiente. A OCI foi fundada em 2015 pela Docker, CoreOS, Google, IBM, Microsoft, Red Hat e VMware e hoje faz parte da Linux Foundation.

O runc, principal projeto desenvolvido pela OCI, √© um container runtime de baixo n√≠vel amplamente utilizado por diversos Container Engines, incluindo o Docker. Este projeto, totalmente open source, √© escrito em Go e seu c√≥digo fonte pode ser acessado no GitHub.

Agora sim j√° podemos falar sobre o que √© o Container Runtime.

#### O Container Runtime

Para que seja poss√≠vel executar os containers nos n√≥s √© necess√°rio ter um *Container Runtime* instalado em cada um desses n√≥s.

O *Container Runtime* √© o respons√°vel por executar os containers nos n√≥s. Quando voc√™ est√° utilizando ferramentas como Docker ou Podman para executar containers em sua m√°quina, por exemplo, voc√™ est√° fazendo uso de algum *Container Runtime*, ou melhor, o seu Container Engine est√° fazendo uso de algum *Container Runtime*.

Temos tr√™s tipos de *Container Runtime*:

- Low-level: s√£o os *Container Runtime* que s√£o executados diretamente pelo Kernel, como o runc, o crun e o runsc.

- High-level: s√£o os *Container Runtime* que s√£o executados por um *Container Engine*, como o containerd, o CRI-O e o Podman.

- Sandbox e Virtualized: s√£o os *Container Runtime* que s√£o executados por um *Container Engine* e que s√£o respons√°veis por executar containers de forma segura. O tipo Sandbox √© executado em unikernels ou utilizando algum proxy para fazer a comunica√ß√£o com o Kernel. O gVisor √© um exemplo de *Container Runtime* do tipo Sandbox. J√° o tipo Virtualized √© executado em m√°quinas virtuais. A performance aqui √© um pouco menor do que quando executado nativamente. O Kata Containers √© um exemplo de *Container Runtime* do tipo Virtualized.

#### Diferen√ßas entre **Control Plane** e **Workers**

 O Control Place √© o c√©rebro, respons√°vel por gerenciar o estado desejado. Ele decide o que o cluster deve rodar, monitor e corrigir desvios.
  O Workeres (N√≥s de trabalho), s√£o os n√≥s onde suas aplica√ß√µes relmente rodam. ele escuta os Pods (containers da sua aplica√ß√£o).

|Aspector | Control Place | Workers |
|---------|---------------|---------|
| Fun√ß√£o| Gerenciar o cluster | Executar aplica√ß√µes (Pods) |
| Responsabilidade| Decis√µes, agendamento, estado do cluster | Rodar containers e servi√ßos |
| Componentes| API Server, etcd, Scheduler, Controller Manager | Kubelet, Kube Proxy, Runtime |
| Onde roda apps?| N√£o (salvo exce√ß√µes em clusters pequenos, ex. minikube) | Sim, aqui ficam os Pods do usu√°rio |


## Principais Conceitos üíª‚Äã
![alt text](image.png)

### Cluster

Conjunto de m√°quinas (f√≠sicas ou virtuais) que trabalham juntas para executar aplica√ß√µes em containers. Um cluster Kubernetes cont√©m pelo menos:

- Um **control plane** (plano de controle)
- Um ou mais **nodes** (n√≥s) onde os aplicativos s√£o executados

---

### Node

√â uma m√°quina (f√≠sica ou virtual) dentro do cluster respons√°vel por executar os **pods**. Existem dois tipos:

- **Master node** (controla o cluster)
- **Worker node** (executa os aplicativos)

---

### Pod

A menor unidade implant√°vel no Kubernetes. Um **pod** pode conter um ou mais containers que compartilham:

- A mesma rede/IP
- Armazenamentos
- Ciclo de vida

---

### Componentes

O Kubernetes √© formado por uma s√©rie de componentes que compartilham um mesmo estado em um banco dedados do tipo Chave-Valor, o ETCD. A intera√ß√£o entre esses diversos componentes gerenciam um cluster que pode ser formado por centenas ou at√© mesmo milhares de containers e servidores. Podemos dividir esses componentes em 3 camadas: Os componentes do Control Plane, os componentes do Node Plane e os Addons.

 - O Control Plane √© respons√°vel pelas decis√µes administrativas do ambiente. Pense nele como a cabe√ßa por tr√°s das decis√µes tomadas pelo Kubernetes.
 - No Node Plane s√£o reunidos os componentes que s√£o executados em todos os servidores do Cluster. Nele temos os componentes que gerenciam os container e rede dos servidores. 
 - Por fim, os Addons s√£o componentes que usam recursos do Kubernetes para adicionar capacidades extras ao Cluster.

![alt text](image-4.png)

##### Kube-apiserver
```
  O Kube-apiserver √© o componente que d√° acesso ao Control Plane e √© a 
  porta de entrada do cluster. Todo acesso ao cluster √© mediado por esta 
  API. Quando executamos um comando atrav√©s do execut√°vel kubectl, 
  estamos fazendo uma chamada a API do Kubernetes, que est√° sendo mantida 
  pelo kube-apiserver.
```
##### ETCD
```
  Todos os dados do Kubernetes s√£o mantidos no ETCD, que √© um banco de dados
  do tipo chave-valor baseado em consenso. Nele o Estado do Cluster √© 
  mantido e por isso o ETCD prioriza a consist√™ncia dos dados sobre 
  disponibilidade.

  E poss√≠vel manter √°rias r√©plicas do ETCD ativas ao mesmo tempo, e mesmo a 
  falha de uma delas n√£o afetar√° a disponibilidade do ambiente e integridade 
  dos dados do Kubernetes.
```

##### Kube-scheduler 
```
    O Kube-scheduler √© o respons√°vel por determinar em que servidor cada 
    pod ser√° iniciado e executado. Para tomar esta decis√£o ele leva em 
    conta diversas quest√µes, como disponibilidade de recursos, pol√≠ticas 
    aplicadas √†s cargas de trabalho e atribui√ß√µes especiais 
    vindas do usu√°rio.
 ```

##### Kube controller manager
```
  O Kube-controller-manager √© respons√°vel por gerir as tarefas 
  administrativas do cluster. Nele, diversos Controladores com diversas 
  responsabilidades distintas s√£o executados a fim de manter o estado do 
  cluster atualizado.
```
##### Kubelet
 ```
    O Kubelet √© o processo respons√°vel por iniciar os containers definidos 
    nos pods. Ele conversa com o Container Runtime instalado no servidor 
    garantindo a execu√ß√£o e a sa√∫de dos containers definidos no Estado do 
    Cluster.
 ```

### Services 

Um dos principais objetivos dos Servi√ßos no Kubernetes √© que voc√™ n√£o precisa modificar sua aplica√ß√£o existente para usar um mecanismo de descoberta de servi√ßos desconhecido. Voc√™ pode executar c√≥digo em Pods, seja um c√≥digo desenvolvido para um ambiente nativo da nuvem ou uma aplica√ß√£o mais antiga que voc√™ conteinerizou. Voc√™ usa um Servi√ßo para **disponibilizar esse conjunto de Pods na rede**, permitindo que os clientes interajam com ele.

**ClusterIP**
Exp√µe o servi√ßo em um IP interno do cluster. Escolher esse valor torna o servi√ßo acess√≠vel apenas de dentro do cluster. Esse √© o valor padr√£o usado se voc√™ n√£o especificar explicitamente um IP para o servi√ßo. Voc√™ pode expor o servi√ßo √† internet p√∫blica usando um Ingress ou um Gateway .
**NodePort**
Exp√µe o servi√ßo no IP de cada n√≥ em uma porta est√°tica (a NodePort). Para disponibilizar a porta do n√≥, o Kubernetes configura um endere√ßo IP de cluster, da mesma forma que se voc√™ tivesse solicitado um servi√ßo de type: ClusterIP.
**LoadBalancer**
Exp√µe o servi√ßo externamente usando um balanceador de carga externo. O Kubernetes n√£o oferece um componente de balanceamento de carga diretamente; voc√™ precisa fornecer um ou pode integrar seu cluster Kubernetes a um provedor de nuvem.
**ExternalName**
Mapeia o servi√ßo para o conte√∫do do externalNamecampo (por exemplo, para o nome do host api.foo.bar.example). O mapeamento configura o servidor DNS do seu cluster para retornar um CNAME registro com esse valor de nome de host externo. Nenhum tipo de proxy √© configurado.

> Comando para pegar a porta do servi√ßo:
` kubectl get svc <nome-service> -o yaml | grep address`

> Lista todos os servi√ßos no namespace atual.
`kubectl get services (ou kubectl get svc)`

> Mostra detalhes completos de um servi√ßo espec√≠fico, incluindo seletores, endpoints e portas.
`kubectl describe service <nome-do-servi√ßo>`

> Cria um servi√ßo para expor um deployment, pod ou outro recurso.
`kubectl expose <tipo-recurso> <nome> --port=<porta> --target-port=<porta>`

> Remove um servi√ßo do cluster.
`kubectl delete service <nome-do-servi√ßo>`

> Lista os endpoints (IP dos pods) aos quais o servi√ßo est√° direcionando.
`kubectl get endpoints <nome-do-servi√ßo>`

> Edita o servi√ßo diretamente no editor de texto.
`kubectl edit service <nome-do-servi√ßo>`

> Faz redirecionamento de porta de um servi√ßo para acesso local.
`kubectl port-forward service/<nome-do-servi√ßo> <porta-local>:<porta-servi√ßo>`

> Altera o tipo do servi√ßo, por exemplo, para NodePort.
`kubectl patch service <nome-do-servi√ßo> -p '{"spec":{"type":"NodePort"}}'`

### Endpoints Controller üìà‚Äã

O Endpoint Controller √© respons√°vel por adicionar Endpoints, ou IP‚Äôs de Pods, a lista de Endpoint de um Service. Ele observa as regras definidas no campo Selector de um Service para descobrir que Pods devem ser adicionados a lista, tornando poss√©vel a descoberta de Servi√ßo de forma f√°cil e pr√°tica usando DNS e Services.


Endpoints no Kubernetes funcionam como um mapa de endere√ßos IP e portas dos pods que pertencem a um servi√ßo, sendo criados automaticamente quando um servi√ßo √© criado.

![alt text](image-5.png)

```yaml
apiVersion: v1     # Vers√£o da API Kubernetes usada para este recurso
kind: Endpoints   # Tipo do recurso: Endpoints, que define IPs backend manualmente
metadata:
  name: my-endpoints-service     # Nome do recurso Endpoints (deve ser o mesmo do Service associado)
subsets:   # Lista de subsets, definindo grupos de endpoints
  - addresses:
      - ip: 77.68.88.76  # IP do backend (aqui um servidor IIS)
#     - ip: 10.244.0.9   # IP comentado (exemplo de outro backend Apache)
#     - ip: 10.244.0.10  # IP comentado (exemplo de backend Nginx)
    ports:
      - port: 80   # Porta em que o backend est√° escutando (porta 80)

---

apiVersion: v1     # Vers√£o da API Kubernetes usada para este recurso
kind: Service     # Tipo do recurso: Service, que exp√µe rede para acesso
metadata:
  name: my-endpoints-service   # Nome do servi√ßo (deve ter mesmo nome do Endpoints para linkar)
spec:
  ports:
    - protocol: TCP     # Protocolo TCP para acesso ao servi√ßo
      port: 80         # Porta exposta pelo servi√ßo no cluster
      targetPort: 80   # Porta no backend para qual o tr√°fego ser√° direcionado

```

##### EndPointSlice
A API EndpointSlice √© o mecanismo que o Kubernetes usa para permitir que seu servi√ßo seja dimensionado para lidar com um grande n√∫mero de backends e permite que o cluster atualize sua lista de backends √≠ntegros de forma eficiente.



##### YAML Kubernetes Example

```yaml

apiVersion: v1
# Indica o tipo do objeto que est√° sendo criado, neste caso um Pod, que √© a menor unidade de implanta√ß√£o que pode conter um ou mais cont√™ineres.
kind: Pod 
# Define metadados do Pod. name √© o nome do Pod que ser√° criado como "rss-site". labels s√£o pares chave-valor que categorizam o Pod
metadata:
  name: rss-site
  labels:
    app: web
# Define a especifica√ß√£o do Pod, ou seja, o que ele deve conter e como ele deve se comportar.
spec:
  containers:
    - name: front-end
      image: nginx-container
      ports:
        - containerPort: 80
    - name: redis-container
      image: redis:5.0.4
      ports:
        - containerPort: 6379

```

### Volume
Volumes fornecem armazenamento persistente para os pods. Eles mant√™m dados mesmo se o pod for recriado
Tipos comuns de volumes:
 - emptyDir
 - hostPath
 - persistentVolumeClaim (PVC)

### Namespace
Permite organizar e isolar recursos dentro de um mesmo cluster. Muito √∫til em ambientes com m√∫ltiplos times ou projetos.
Exemplo: `kubectl create namespace meu-projeto`

### Objetos do Kubernetes
Os objetos principais do Kubernetes representam recursos persistentes que definem o estado desejado do cluster e das aplica√ß√µes que nele rodam. Entre os mais importantes est√£o:
- Deployment: Gerencia a implanta√ß√£o e o ciclo de vida de um conjunto de r√©plicas de pods. Garante o n√∫mero desejado de r√©plicas, possibilita atualiza√ß√µes e rollback de vers√µes da aplica√ß√£o, facilitando escalabilidade e alta disponibilidade.

Exemplo de defini√ß√£o:
```yaml
apiVersion: apps/v1         # Define a vers√£o da API do Kubernetes que ser√° usada para esse recurso (aqui: apps/v1, usado para Deployments).
kind: Deployment            # Indica o tipo de recurso que est√° sendo criado, neste caso um Deployment.
metadata:
  name: meu-app             # Nome do Deployment, que serve como identificador dentro do cluster.
spec:
  replicas: 3               # Quantidade de r√©plicas (pods) que o Deployment deve manter em execu√ß√£o.
  selector:                 # Define como o Deployment vai identificar quais Pods pertencem a ele.
    matchLabels:            # Crit√©rio de correspond√™ncia: procura Pods com o r√≥tulo abaixo.
      app: meu-app          # R√≥tulo usado para identificar os Pods que fazem parte deste Deployment.
  template:                 # Template que descreve como os Pods desse Deployment devem ser criados.
    metadata:
      labels:
        app: meu-app        # R√≥tulo que ser√° adicionado aos Pods criados (precisa bater com o selector acima).
    spec:
      containers:           # Lista de containers que ir√£o rodar dentro de cada Pod.
        - name: nginx       # Nome do container (apenas identificador dentro do Pod).
          image: nginx      # Imagem Docker que ser√° usada para criar o container (nesse caso, a imagem oficial do Nginx do Docker Hub).
          ports:
            - containerPort: 80  # Porta exposta pelo container (nesse caso, o Nginx escuta na porta 80).

```

- DaemonSet: Garante que uma c√≥pia de um pod espec√≠fico seja executada em todos (ou em n√≥s selecionados) do cluster. Muito usado para tarefas como coleta de logs ou monitoramento que precisam rodar em cada n√≥.
``` bash
kubectl get daemonset # lista os DaemonSets presentes no cluster e mostra status b√°sico.
kubectl describe daemonset <nome> # mostra detalhes completos e eventos relacionados ao DaemonSet especificado.
kubectl apply -f <arquivo.yaml> # cria ou atualiza um DaemonSet a partir de um arquivo de configura√ß√£o YAML.
kubectl create -f <arquivo.yaml> # cria um DaemonSet a partir do YAML.
kubectl delete daemonset <nome> # exclui o DaemonSet especificado.
kubectl edit daemonset <nome> # abre o DaemonSet para edi√ß√£o direta no editor de texto.
kubectl rollout status daemonset/<nome> # mostra o status da atualiza√ß√£o de rollout do DaemonSet.
kubectl label nodes <nome-do-no> <r√≥tulo> # adiciona r√≥tulos a n√≥s para limitar onde o DaemonSet ser√° executado, usando seletores de n√≥.
```
- ReplicaSet √© o controlador de n√≠vel intermedi√°rio que mant√©m a quantidade desejada de pods id√™nticos sempre ativos no cluster Kubernetes.
- Job: Cria pods que executam tarefas √∫nicas ou batch, garantindo que elas sejam conclu√≠das com sucesso uma vez. Finaliza ap√≥s a conclus√£o da tarefa.
```bash
kubectl create job <nome> --image=<imagem> # cria um Job rapidamente usando a imagem especificada.
kubectl apply -f <arquivo.yaml> # cria ou atualiza um Job a partir de um arquivo YAML.
kubectl get jobs # lista todos os Jobs no namespace atual.
kubectl describe job <nome> # exibe detalhes completos de um Job espec√≠fico, incluindo status e eventos.
kubectl delete job <nome> # deleta um Job espec√≠fico.
kubectl logs job/<nome> # exibe os logs dos Pods associados ao Job.
kubectl get pods --selector=job-name=<nome> # lista os Pods criados por um Job.
kubectl rollout status job/<nome> # acompanha o status de rollout (geralmente para Jobs controlados por controllers).
```
- CronJob: Permite agendar Jobs para execu√ß√£o peri√≥dica, similar a um cron do Linux. Ideal para tarefas como backups, atualiza√ß√µes ou verifica√ß√µes programadas.

```yaml

apiVersion: batch/v1   # [translate:O c√≥digo YAML que voc√™ apresentou define um recurso Kubernetes CronJob, que agenda a execu√ß√£o peri√≥dica de Jobs (tarefas) no cluster. Explicando cada parte:]
kind: CronJob    # apiVersion: batch/v1¬†e¬†kind: CronJob¬†indicam que √© um recurso do tipo CronJob na API batch/v1.
metadata: # metadata.name: my-cj¬†define o nome do CronJob.
  name: my-cj
spec:
  failedJobsHistoryLimit: 5    # spec.failedJobsHistoryLimit: 5¬†mant√©m o hist√≥rico dos √∫ltimos 5 Jobs que falharam.
  successfulJobsHistoryLimit: 10    # spec.successfulJobsHistoryLimit: 10¬†mant√©m o hist√≥rico dos √∫ltimos 10 Jobs que foram bem-sucedidos.
  schedule: "* * * * *"   # spec.schedule: "* * * * *"¬†indica que o Job ser√° disparado a cada minuto, no formato cron (minuto, hora, dia do m√™s, m√™s, dia da semana).
  jobTemplate:   # spec.jobTemplate¬†√© o template que descreve o Job que ser√° criado a cada execu√ß√£o:
    spec:
      completions: 10   # completions: 10¬†define que o Job deve completar 10 pods com sucesso para ser considerado conclu√≠do.
      completionMode: "Indexed"   # completionMode: "Indexed"¬†indica que cada pod ter√° um √≠ndice √∫nico, √∫til para trabalhos paralelos.
      template:
        spec:
          containers:  # template.spec¬†cont√©m a especifica√ß√£o do Pod que ser√° criado:
          - name: my-container-busybox # Um container chamado¬†my-container-busybox¬†com a imagem¬†busybox.
            image: busybox
            command:
            - "/bin/sh" # Executa o comando¬†/bin/sh -c "for i in 1 2; do echo Lucky number $1 = $((1 + RANDOM % 70)); done", que imprime dois n√∫meros aleat√≥rios.
            - "-c"
            - "for i in 1 2; do echo Lucky number $1 = $((1 + RANDOM % 70)); done"
          restartPolicy: Never    # restartPolicy: Never¬†indica que o Pod n√£o ser√° reiniciado em caso de falha.


# Esse CronJob executa, a cada minuto, um Job que cria 10 pods paralelos (devido a completions: 10 e completionMode: Indexed), cada um executando o comando que gera n√∫meros aleat√≥rios.
```

Esses objetos s√£o configurados via arquivos YAML declarativos, usados para definir o estado desejado que o Kubernetes mant√©m automaticamente, criando, atualizando ou removendo pods conforme necess√°rio para atender √†s especifica√ß√µes

## Comandos B√°sicos com kubectl
```bash
# Listar pods
kubectl get pods

# Listar servi√ßos
kubectl get services

#  Mostra m√©tricas de uso de CPU e mem√≥ria dos pods.
kubectl top pod

# Aplicar um arquivo YAML
kubectl apply -f deployment.yaml

# Deletar um recurso
kubectl delete pod nome-do-pod 
# ou
kubectl delete pod my-pod --now

# Ver logs de um pod
kubectl logs nome-do-pod

# Executar comando dentro do pod
kubectl exec -it nome-do-pod -- bash

# Listar todos os pods de todos os namespaces
kubectl get pods --all-namespaces

# Aumentar o n√∫mero de pods para 5 em um Deployment chamado minha-aplicacao
kubectl scale --replicas=5 deployment/minha-aplicacao

# Para descrever um pod chamado my-pod-webserver:
kubectl describe pod my-pod-webserver

# Exibir o hist√≥rico de revis√µes de uma implanta√ß√£o (Deployment), mostrando as 
# vers√µes anteriores e as mudan√ßas realizadas em cada uma delas.
kubectl rollout history deployment/nome-do-deployment

# Mostra logs do container my-container em uma inst√¢ncia anterior do pod.
kubectl logs my-pod -c my-container --previous

# Comando para conectar a um POD no kubernetes.
kubectl exec -it <nome-do-pod> -- /bin/bash

# Comando para listar namespaces
kubectl get ns

# Comando para criar um namespace
kubectl create namespace <nome-namespace> --save-config

# Comando para criar um pod dentro de um determinado namespace
kubectl run <nome-do-pod> --image=<imagem> --namespace=<nome-do-namespace>
# ou
kubectl apply -f meu-pod.yaml --namespace=<nome-do-namespace>

# Encaminha a porta 8080 local para a porta 80 do service.
kubectl port-forward svc/my-service 8080:80

```

O comando `kubectl rollout pause` √© usado para pausar o rollout (implanta√ß√£o) de um recurso no Kubernetes, normalmente um Deployment. Quando um rollout est√° pausado, o controlador n√£o realiza altera√ß√µes ou atualiza√ß√µes no recurso, mesmo que haja mudan√ßas aplicadas no manifesto. Isso √© √∫til para interromper temporariamente atualiza√ß√µes autom√°ticas, permitindo que o administrador aplique v√°rias modifica√ß√µes antes de retomar a implanta√ß√£o com o comando kubectl rollout resume.
EX: Pausar o rollout de um Deployment chamado nginx: `kubectl rollout pause deployment/nginx`

![alt text](image-2.png)

## Portas utilizados pelos componentes do Kubernetes

  - 6443 (TCP) ‚Üí entrada no cluster (API Server)
  - 2379‚Äì2380 (TCP) ‚Üí armazenamento do estado (etcd)
  - 10250 (TCP) ‚Üí comunica√ß√£o entre API Server e n√≥s (Kubelet)
  - 10251 (TCP) ‚Üí scheduler
  - 10252 (TCP) ‚Üí controller manager

  > Uma porta TCP √© um n√∫mero que funciona como um ponto de entrada e sa√≠da numa rede, associado a um aplicativo espec√≠fico num dispositivo


### Livesprobe

Uma liveness probe no Kubernetes √© uma verifica√ß√£o peri√≥dica da sa√∫de de um container rodando dentro de um pod. Ela serve para garantir que o aplicativo est√° funcionando corretamente, e, se detectar que o container est√° travado, n√£o responde ou n√£o est√° mais operacional, o Kubernetes automaticamente reinicia esse container para restaurar sua funcionalidade.
> Executa verifica√ß√µes em intervalos definidos, usando m√©todos como requisi√ß√µes HTTP, testes TCP ou comandos espec√≠ficos.
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: liveness-example
spec:
  containers:
  - name: demo-container
    image: k8s.gcr.io/liveness
    livenessProbe:
      httpGet:
        path: /healthz
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 10
      timeoutSeconds: 2
      failureThreshold: 3
```
Explica√ß√£o:

- A livenessProbe faz uma requisi√ß√£o HTTP GET para o caminho /healthz na porta 8080 do container.
- Inicialmente espera 15 segundos ap√≥s o container iniciar para come√ßar a verificar.
- Depois, realiza a verifica√ß√£o a cada 10 segundos.
Considera falha a sonde se n√£o responder dentro de 2 segundos.
- Se falhar 3 vezes consecutivas, o Kubernetes reiniciar√° o container automaticamente.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: liveness-pod
spec:
  containers:
  - name: liveness-container-test
    image: busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600
    
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5
      failureThreshold: 3
```
Explica√ß√£o:
Esse arquivo YAML define um Pod no Kubernetes com um container que inclui uma livenessProbe, usada para verificar se o container est√° "vivo" e funcionando corretamente.
- A livenessProbe usa um comando cat /tmp/healthy para verificar a exist√™ncia desse arquivo.
- A sondagem come√ßa ap√≥s um initialDelaySeconds de 5 segundos, e √© repetida a cada 5 segundos (periodSeconds).
- Se o comando falhar (por exemplo, porque o arquivo /tmp/healthy n√£o existe), ap√≥s 3 falhas consecutivas (failureThreshold), o Kubernetes considera que o container est√° com problema.
- Nesse caso, o kubelet ir√° reiniciar o container automaticamente para tentar restaurar o estado saud√°vel.

### Requests
Requests e Limits: Cada container pode declarar a quantidade m√≠nima (request) e m√°xima (limit) de recursos que pode usar

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: resources-pod
spec:
  containers:
    - name: apache-container
      image: httpd

      resources:
        requests:
          cpu: "500m"
          memory: "128Mi"
```
> Exemplo de especifica√ß√£o de requests e limits em um Pod:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: resource-demo-pod
spec:
  containers:
  - name: app-container
    image: nginx
    resources:
      requests:
        memory: "256Mi"
        cpu: "500m"
      limits:
        memory: "512Mi"
        cpu: "1"
```

### Volumes

No Kubernetes, volumes s√£o uma abstra√ß√£o que permitem que cont√™ineres dentro de um Pod armazenem e compartilhem dados via sistema de arquivos. Diferente do sistema de arquivos ef√™mero dos containers, que √© destru√≠do junto com o container, os volumes podem ter vida √∫til maior, persistindo al√©m da vida do Pod, dependendo do tipo.

- Um volume √© declarado na se√ß√£o .spec.volumes do Pod.
- √â montado em um ou mais containers do Pod na se√ß√£o .spec.containers[].volumeMounts, definindo o caminho onde o volume ficar√° vis√≠vel dentro do container.

O volume **emptyDir** no Kubernetes √© um volume tempor√°rio criado no momento em que um Pod √© atribu√≠do a um n√≥ e que existe enquanto o Pod est√° ativo naquele n√≥. Inicialmente vazio, ele √© compartilhado entre todos os containers do Pod e seu conte√∫do √© apagado definitivamente quando o Pod √© removido

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: emptydir-example
spec:
  containers:
  - name: mycontainer
    image: busybox
    command: ['sh', '-c', 'sleep 3600']
    volumeMounts:
    - name: cache-volume
      mountPath: /cache
  volumes:
  - name: cache-volume
    emptyDir: {}

# O volume cache-volume √© do tipo emptyDir e montado em /cache no container.
  ```

O volume **hostPath** no Kubernetes √© um tipo de volume que monta um diret√≥rio ou arquivo do sistema de arquivos do n√≥ (host) onde o pod est√° sendo executado diretamente dentro do container. √â √∫til para acessar dados locais do n√≥, como logs, arquivos tempor√°rios ou outras informa√ß√µes espec√≠ficas do host.

> Exemplo de uso de volume hostPath em um pod:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: hostpath-demo-pod
spec:
  containers:
  - name: container-demo
    image: busybox
    command: ["sh", "-c", "sleep 3600"]
    volumeMounts:
    - name: host-volume
      mountPath: /mnt/host
  volumes:
  - name: host-volume
    hostPath:
      path: /tmp
      type: Directory
```

### Persistent Volume (PV e PVC)

O **Persistent Volume (PV)** no Kubernetes √© um recurso de armazenamento previamente provisionado no cluster, que pode ser configurado pelo administrador para fornecer espa√ßo de armazenamento abstrato e persistente para uso dos pods. Ele possui propriedades como tamanho, modo de acesso (ex: ReadWriteOnce, ReadOnlyMany, ReadWriteMany) e pode estar em estados como Available (dispon√≠vel), Bound (associado) ou Released (liberado).
> EX
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-example
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: "/mnt/data"
```
J√° o **Persistent Volume Claim (PVC)** √© uma requisi√ß√£o feita pelo usu√°rio/pod para solicitar armazenamento. O PVC descreve suas necessidades como tamanho, modo de acesso e classe de armazenamento, e o sistema do Kubernetes realiza o bind (associa√ß√£o) autom√°tico a um PV que atenda aos crit√©rios requisitados. O usu√°rio interage com o PVC para usar o armazenamento, sem se preocupar com a implementa√ß√£o do PV.
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-example
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
```

Com esse PVC, o Kubernetes automaticamente liga esse pedido a um PV que tenha pelo menos 5Gb de espa√ßo e o modo de acesso requisitado, permitindo o pod usar esse volume via a claim.

Assim, o PV representa o recurso f√≠sico de armazenamento, enquanto o PVC √© a forma de requisitar e usar esse recurso no Kubernetes, promovendo abstra√ß√£o e melhor gerenciamento do armazenamento em clusters.

#### Direction node attribution
A "direction node attribution" (atribui√ß√£o ou direcionamento de pods para n√≥s) no Kubernetes √© realizada principalmente por meio de mecanismos de sele√ß√£o e afinidade baseados em labels dos n√≥s.

Os principais m√©todos para direcionar a atribui√ß√£o de pods a n√≥s espec√≠ficos s√£o:

- Node Selector: Uma forma simples que usa labels dos n√≥s para agendar um pod apenas em n√≥s com determinados labels. Por exemplo, um pod pode ter nodeSelector com disktype=ssd para ser executado somente em n√≥s com esse label.
- Node Affinity: Um m√©todo mais flex√≠vel e sofisticado que permite expressar regras de afinidade (prefer√™ncia) ou anti-afinidade baseadas em labels e condi√ß√µes mais complexas, incluindo operadores como In, NotIn, Exists etc.
- Taints and Tolerations: Os n√≥s podem ser "tintados" (tainted) para repelir pods que n√£o tenham as correspondentes "tolerations", ajudando a controlar onde pods podem ou n√£o ser executados.
- Pod affinity/anti-affinity: Permite definir regras para que pods prefiram ou evitem rodar perto de outros pods com determinadas caracter√≠sticas, influenciando indireta e dinamicamente a sele√ß√£o do n√≥.

Exemplo 1: Node Affinity obrigat√≥ria (requiredDuringSchedulingIgnoredDuringExecution)
Este pod ser√° agendado apenas em n√≥s que tenham o label disktype: ssd:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-com-node-affinity-obrigatoria
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
  containers:
  - name: nginx
    image: nginx
```
#### Node labels

Node labels no Kubernetes s√£o pares chave-valor associados aos n√≥s (m√°quinas f√≠sicas ou virtuais) do cluster. Esses labels ajudam a categorizar e identificar os n√≥s com informa√ß√µes espec√≠ficas, como localiza√ß√£o geogr√°fica, tipo de hardware, ambiente (produ√ß√£o, desenvolvimento), etc. 
Eles s√£o usados principalmente pelo scheduler do Kubernetes para decidir onde agendar os pods

> Exemplo de comando para adicionar label a um n√≥:
`kubectl label nodes node-1 disktype=ssd`

Isso adiciona o label disktype=ssd ao n√≥ node-1. Depois, um pod pode usar um nodeSelector para ser agendado somente em n√≥s com esse label.

> Comando para listar as labels de um n√≥ no Kubernetes:
`kubectl get node <nome-do-no> --show-labels`

> comando para listar apenas os nomes dos n√≥s em um cluster:
`kubectl get nodes -o name`


#### Restart policy application kubernetes

Os Pods do Kubernetes possuem um restartPolicycampo specque define como os cont√™ineres do Pod s√£o reinicializados em caso de encerramento. Essa pol√≠tica se aplica a todos os cont√™ineres dentro do Pod.

**Pol√≠ticas de reinicializa√ß√£o dispon√≠veis:**
- Always (Default): Essa pol√≠tica instrui o Kubernetes a sempre reiniciar os cont√™ineres do Pod quando eles forem encerrados, independentemente de terem sido finalizados com sucesso ou com erro. Isso √© adequado para servi√ßos de longa dura√ß√£o, como servidores web.
- OnFailure: Essa pol√≠tica reinicia os cont√™ineres do Pod somente se eles forem encerrados com um c√≥digo de sa√≠da diferente de zero, indicando uma falha. Isso geralmente √© usado para trabalhos em lote ou processos que podem ser repetidos em caso de falha. 
- Never: Essa pol√≠tica impede que o Kubernetes reinicie os cont√™ineres do Pod ap√≥s o t√©rmino da execu√ß√£o, independentemente do status de sa√≠da. Isso √© apropriado para aplica√ß√µes projetadas para serem executadas uma √∫nica vez e depois paradas.

Exemplo:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: example-pod
spec:
  restartPolicy: OnFailure   # Pol√≠tica de rein√≠cio: reinicia o container somente se ele falhar (exit code != 0)
  containers:
  - name: example-container
    image: busybox
    command: ["sh", "-c", "echo Hello Kubernetes; sleep 30; exit 1"]
```

#### JOBS

Kubernetes Jobs s√£o recursos usados para executar tarefas finitas, de curta dura√ß√£o, uma √∫nica vez e que precisam ser conclu√≠das com sucesso. Eles criam um ou mais Pods para executar a tarefa, monitoram sua execu√ß√£o, e garantem que o n√∫mero desejado de execu√ß√µes com sucesso seja alcan√ßado. Depois disso, os Pods s√£o finalizados e o Job √© marcado como completo.

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: example-job
spec:
  template:
    spec:
      containers:
      - name: example
        image: busybox
        command: ["echo", "Hello, World!"]
      restartPolicy: Never #  Essa pol√≠tica impede que o Kubernetes reinicie os cont√™ineres do Pod ap√≥s o t√©rmino da execu√ß√£o
```

#### Config Map

Um ConfigMap √© um objeto de API usado para armazenar dados n√£o confidenciais em pares de chave-valor. C√°psulaspode consumir ConfigMaps como vari√°veis ‚Äã‚Äãde ambiente, argumentos de linha de comando ou como arquivos de configura√ß√£o em um volume.

### Secrets

No Kubernetes, Secrets s√£o objetos usados para armazenar e gerenciar informa√ß√µes sens√≠veis, como senhas, tokens, chaves SSH e certificados, de forma segura dentro do cluster. Eles evitam que dados confidenciais fiquem expostos em arquivos de configura√ß√£o ou imagens de containers. Secrets podem ser consumidos pelos Pods como vari√°veis de ambiente, arquivos montados em volumes ou usados internamente pelo sistema, por exemplo, para autentica√ß√£o.

> Consumir um Secret como vari√°veis de ambiente num Pod:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: secret-env-pod
spec:
  containers:
  - name: mycontainer
    image: redis
    env:
    - name: SECRET_USERNAME
      valueFrom:
        secretKeyRef:
          name: mysecret
          key: username
    - name: SECRET_PASSWORD
      valueFrom:
        secretKeyRef:  # Serve para referenciar uma chave espec√≠fica dentro de um Secret
          name: mysecret
          key: password
```

### StatefulSet no Kubernetes

- **O que √©:**  
  StatefulSet √© um recurso do Kubernetes usado para gerenciar aplica√ß√µes stateful (com estado), garantindo que cada pod tenha uma identidade √∫nica e persistente durante seu ciclo de vida.

- **Caracter√≠sticas principais:**  
  - Cada pod tem um nome e identidade est√°veis (hostname fixo).  
  - Volumes persistentes s√£o associados individualmente a cada pod, preservando dados entre reinicializa√ß√µes.  
  - Pilha de cria√ß√£o e exclus√£o em ordem definida (ex.: pod-0 antes do pod-1).  
  - Permite escalonar com controle e previsibilidade para workloads que exigem estado consistente.

- **Quando usar:**  
  Aplica√ß√µes que necessitam de armazenamento persistente, como bancos de dados (MySQL, PostgreSQL), caches com estado (Redis), e sistemas distribu√≠dos que dependem de identidade e armazenamento est√°vel.

- **Diferen√ßa para Deployment:**  
  Deployments s√£o para aplica√ß√µes stateless e n√£o garantem identifica√ß√£o fixa dos pods; j√° StatefulSets mant√™m identidades e armazenamento persistente.

- **Exemplo de uso:**  
  Gerenciar um cluster Redis com r√©plicas que precisam de armazenamento separado e identifica√ß√£o est√°vel para replica√ß√£o e failover.

Em resumo, StatefulSets s√£o essenciais para cargas de trabalho que mant√™m estado e precisam ser executadas com alta disponibilidade e consist√™ncia dentro do ambiente Kubernetes.

### RBAC (Role-Based Access Control)

RBAC (Role-Based Access Control) no Kubernetes √© um sistema de controle de acesso que define quem pode fazer o qu√™ dentro do cluster com base nas fun√ß√µes atribu√≠das a usu√°rios, grupos ou contas de servi√ßo. Ele gerencia permiss√µes para acessar e operar recursos do Kubernetes, como pods, servi√ßos, entre outros, garantindo seguran√ßa e organiza√ß√£o no ambiente. O RBAC utiliza quatro recursos principais: 

- Roles (definem permiss√µes dentro de um namespace), 
- ClusterRoles (definem permiss√µes para o cluster inteiro),
- RoleBindings (associam Roles a usu√°rios ou grupos dentro de um namespace) e 
- ClusterRoleBindings (associam ClusterRoles a usu√°rios ou grupos para todo o cluster). 

Essa estrutura possibilita um gerenciamento granular e flexible das permiss√µes, facilitando a aplica√ß√£o do princ√≠pio do menor privil√©gio e melhorando a seguran√ßa do cluster.

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role            # Define uma Role que encapsula permiss√µes 
metadata:
  namespace: default  # Namespace onde essa Role ser√° aplicada
  name: auditor-role
rules:
- apiGroups: [""]     # Grupo de API vazio significa o grupo core do Kubernetes
  resources: ["pods","services"]  # Recursos do Kubernetes aos quais essa Role ter√° acesso
  verbs: ["get","list","watch"]   # A√ß√µes permitidas sobre esses recursos: leitura e monitoramento

# Role √© um objeto de RBAC (Role-Based Access Control) que define um conjunto
# de permiss√µes dentro de um namespace espec√≠fico. Ela especifica quais a√ß√µes 
# (como criar, listar, atualizar, deletar) um usu√°rio, grupo ou conta de servi√ßo 
# pode realizar em quais recursos (como pods, servi√ßos, deployments) dentro 
# daquele namespace.
---

apiVersion: rbac.authorization.k8s.io/v1  # Vers√£o da API para recursos RBAC
kind: RoleBinding  # Define um RoleBinding que associa uma Role a usu√°rios, grupos ou contas de servi√ßo
metadata:
  namespace: default   # Namespace onde essa associa√ß√£o ocorrer√° (deve ser o mesmo da Role)
  name: auditor-rb   # Nome do RoleBinding, para identificar esta associa√ß√£o
subjects:
- kind: User     # Tipo de sujeito que receber√° a permiss√£o (aqui um usu√°rio)
  name: auditor  # Nome do usu√°rio que receber√° a permiss√£o concedida pela Role
  apiGroup: rbac.authorization.k8s.io  # APIGroup para RBAC
roleRef:
  kind: Role    # Refer√™ncia √† Role que ser√° associada
  name: auditor-role  # Nome da Role que ser√° vinculada ao sujeito
  apiGroup: rbac.authorization.k8s.io  # APIGroup para RBAC

```
> Comando para alterar o contexto ativo:
`kubectl config use-context <user_conectext>`

<KBD>IMPORTANTE!</KBD> Em resumo, `kubectl config use-context` troca o contexto atual, o que inclui o usu√°rio do kubeconfig que ser√° utilizado, mas o foco do comando √© na troca completa do contexto (cluster, namespace e usu√°rio), n√£o apenas do usu√°rio isoladamente

> Comando para visualizar o contexto atual usado pelo kubectl.
`kubectl config current-context`

#### Gerenciar rollouts e implanta√ß√µes.

> Comando para reverter um deployment
`kubectl rollout undo deployment/<nome-do-pod>`

> Reverte o deployment para uma revis√£o espec√≠fica anterior.
`kubectl rollout undo deployment/<deployment-name> --to-revision=<revision-number>`

> Exibe o status do rollout atual para verificar progresso e poss√≠veis erros.
`kubectl rollout status deployment/<deployment-name>`

> Lista o hist√≥rico de revis√µes do deployment para saber quais vers√µes foram implantadas.
`kubectl rollout history deployment/<deployment-name>`

> Reinicia todos os pods do deployment for√ßando um novo rollout sem alterar a imagem.
`kubectl rollout restart deployment/<deployment-name>`

> Pausa um rollout ativo para que possa ser revisado ou corrigido antes de continuar.
`kubectl rollout pause deployment/<deployment-name>`

> Retoma um rollout pausado.
`kubectl rollout resume deployment/<deployment-name>`